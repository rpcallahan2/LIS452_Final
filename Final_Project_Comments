	I originally thought the biggest part of my programming project would be copying abstracts from the library catalogue into .txt. files.  However, about halfway through one issue of Foreign Affairs, I found out I could download both citation data and abstracts using scopus.  Unfortunately, Foreign Affairs did not have abstracts on scopus, so I changed my test case journal to Survival.  Having gathered my test data, I began creating my tool.  Currently, the tool is contingent on receiving a .csv file with scopus's column headers.  A potential improvment would be to iterate through the header row and use regular expressions and accumulator variables to identify which columns have the information I am interested in.  First, the program reads in the file and creates a csv object.  Then, it reads through each row in the table creating a dictionary entry whose key is the article's title and whose value is a tuple of the article's citation count and abstract.  
	After creating my dictionary of title, abstract, and citation metric, I originally tokenized the abstract using the Natural Language Tool Kit's nltk.word_tokenzier method.  I had some trouble with tokenizing the abstracts because some abstracts were copyrighted and included the copyright symbol (which isn't compatible with word_tokenizer).  I manually removed the copyright statements, but I could improve the program by using a try, except framework to somehow identify the noncompliant symbol and remove it from the abstract.  
	I then ran readability's readability.getmeasure method on the tokenized abstract to get nested dictionaries of readability scores.  Since the output of readability is standardized, I hardcoded the dictionary keys to access the FRE score for each title.  I then created lists of each article's citation metric and FRE score.  Finally, I turned these lists into arrays, and used scipy's stats.linregress method on the arrays to perform a linear regression on the relationship between FRE and citations.  
	Initially, the statistical analysis suggested the relationship between FRE and citations was random, which would not have agreed with the literature.  I decided to check whether I was conducting the analysis correctly.  In order to do so, I printed the tokenized text and realized nltk.word_tokenize tokenizes text into individual words but readability.getmeasure expects text to be tokenized into sentences.  Therefore, I switched tokenizers to tokenize_uk.tokenize_sents. This method gave readability.getmeasure the input I expected, but threw some additional errors.  This required me to move the tokenizer into the try: statement.  
	For some reason, changing the tokenizer meant that I couldn't call stats.linregress using scipy.stats.linregress, so instead I imported stats from scipy instead of importing scipy and calling stats through scipy.  After getting the regression to run, the statistical metrics were no longer indicative of a random relationship which leads me to believe I have a working tool for examining the relationship between some ranking (citation metrics in this case) and the readability (FRE in this case) of a group of texts.  
